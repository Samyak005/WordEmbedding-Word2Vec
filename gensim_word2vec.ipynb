{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['place', 'the', 'stock', 'lentils', 'celery', 'carrot', 'thyme', 'and', 'salt', 'in', 'a', 'medium', 'saucepan', 'and', 'bring', 'to', 'a', 'boil']\n",
      "['reduce', 'heat', 'to', 'low', 'and', 'simmer', 'until', 'the', 'lentils', 'are', 'tender', 'about', 'minutes', 'depending', 'on', 'the', 'lentils']\n",
      "['if', 'they', 'begin', 'to', 'dry', 'out', 'add', 'water', 'as', 'needed']\n",
      "['remove', 'and', 'discard', 'the', 'thyme']\n",
      "['drain', 'and', 'transfer', 'the', 'mixture', 'to', 'a', 'bowl', 'let', 'cool']\n",
      "[]\n",
      "['fold', 'in', 'the', 'tomato', 'apple', 'lemon', 'juice', 'and', 'olive', 'oil']\n",
      "['season', 'with', 'the', 'pepper']\n",
      "[]\n",
      "['to', 'assemble', 'a', 'wrap', 'place', 'lavash', 'sheet', 'on', 'a', 'clean', 'work', 'surface']\n",
      "['spread', 'some', 'of', 'the', 'lentil', 'mixture', 'on', 'the', 'end', 'nearest', 'you', 'leaving', 'a', 'border']\n",
      "['top', 'with', 'several', 'slices', 'of', 'turkey', 'then', 'some', 'of', 'the', 'lettuce']\n",
      "['roll', 'up', 'the', 'lavash', 'slice', 'crosswise', 'and', 'serve']\n",
      "['if', 'using', 'tortillas', 'spread', 'the', 'lentils', 'in', 'the', 'center', 'top', 'with', 'the', 'turkey', 'and', 'lettuce', 'and', 'fold', 'up', 'the', 'bottom', 'left', 'side', 'and', 'right', 'side', 'before', 'rolling', 'away', 'from', 'you']\n",
      "['lentil', 'apple', 'and', 'turkey', 'wrap', 'cups', 'low-sodium', 'vegetable', 'or', 'chicken', 'stock', 'cup', 'dried', 'brown', 'lentils', 'cup', 'dried', 'french', 'green', 'lentils', 'stalks', 'celery', 'chopped', 'large', 'carrot', 'peeled', 'and', 'chopped', 'sprig', 'fresh', 'thyme', 'teaspoon', 'kosher', 'salt', 'medium', 'tomato', 'cored', 'seeded', 'and', 'diced', 'small', 'fuji', 'apple', 'cored', 'and', 'diced', 'tablespoon', 'freshly', 'squeezed', 'lemon', 'juice', 'teaspoons', 'extra-virgin', 'olive', 'oil', 'ground', 'black', 'pepper', 'to', 'taste', 'sheets', 'whole-wheat', 'lavash', 'cut', 'in', 'half', 'crosswise', 'or', 'flour', 'tortillas', 'pound', 'turkey', 'breast', 'thinly', 'sliced', 'head', 'bibb', 'lettuce', 'first', 'ingredients', 'in', 'heavy', 'medium', 'saucepan']\n",
      "['add', 'shallots']\n",
      "['bring', 'to', 'simmer']\n",
      "['remove', 'from', 'heat', 'cover', 'and', 'let', 'stand', 'minutes']\n",
      "['chill', 'overnight']\n"
     ]
    }
   ],
   "source": [
    "enc = 'utf-8'\n",
    "textfile = open('array.txt','r', encoding=enc)\n",
    "filetext = textfile.read()\n",
    "filetext = filetext.lower()\n",
    "textfile.close()\n",
    "sentences = sent_tokenize(filetext)\n",
    "# print(sentences)\n",
    "word_tokenized = [word_tokenize(t) for t in sentences]\n",
    "\n",
    "for i in range(len(word_tokenized)):\n",
    "    word_tokenized[i] = [word.lower() for word in word_tokenized[i] if re.match('^[a-zA-Z]+', word)]\n",
    "\n",
    "for i in range(20):\n",
    "    print(word_tokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-260e312793ad>:6: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  model.most_similar('pasta')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spaghetti', 0.7611843347549438),\n",
       " ('linguine', 0.7305552959442139),\n",
       " ('fusilli', 0.729653000831604),\n",
       " ('orzo', 0.7136660814285278),\n",
       " ('farfalle', 0.698258638381958),\n",
       " ('fettuccine', 0.6774977445602417),\n",
       " ('penne', 0.6750302910804749),\n",
       " ('rotini', 0.6708777546882629),\n",
       " ('ravioli', 0.6544106006622314),\n",
       " ('orecchiette', 0.6519646644592285)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(sentences = word_tokenized, size = 100, sg = 1, window = 3, min_count = 1, iter = 10, workers = 8)\n",
    "model.init_sims(replace = True)\n",
    "\n",
    "model.save('word2vec_model')\n",
    "model = Word2Vec.load('word2vec_model')\n",
    "model.most_similar('pasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tomatoes', 0.7262853384017944),\n",
       " ('plum', 0.6687163710594177),\n",
       " ('green-olive', 0.654394268989563),\n",
       " ('yogurt-spice', 0.6434577703475952),\n",
       " ('tomatillo', 0.6383132934570312),\n",
       " ('skordalia', 0.6376282572746277),\n",
       " ('tomato-cream', 0.6342430710792542),\n",
       " ('chermoula', 0.628291130065918),\n",
       " ('arrabiata', 0.6280500888824463),\n",
       " ('criolla', 0.6274158954620361)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('tomato')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('granulated', 0.7184647917747498),\n",
       " ('honey', 0.6953933238983154),\n",
       " ('cornstarch', 0.6411632895469666),\n",
       " ('teaspoon\\\\xa0water', 0.6301073431968689),\n",
       " ('piloncillo', 0.6284873485565186),\n",
       " ('cup/70g', 0.624285101890564),\n",
       " ('g/3.3', 0.6188865900039673),\n",
       " ('unbeaten', 0.615382730960846),\n",
       " ('g/6.3', 0.6145912408828735),\n",
       " ('unsulfured', 0.6132355332374573)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('sugar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cayenne', 0.7426214814186096),\n",
       " ('kosher', 0.6941984295845032),\n",
       " ('aleppo-style', 0.6791687607765198),\n",
       " ('teaspoon', 0.6734120845794678),\n",
       " ('slat', 0.6620815992355347),\n",
       " ('mace', 0.6532929539680481),\n",
       " ('paprikÂ´s', 0.6500686407089233),\n",
       " ('milled', 0.6489173173904419),\n",
       " ('papper', 0.6433277130126953),\n",
       " ('fresh-ground', 0.6369504928588867)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('salt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "gensim_word2vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
